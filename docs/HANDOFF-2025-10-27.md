# Spespe Handoff — 2025-10-27 (Europe/Rome)

## Self-Report — Current State
- Added env tooling (`scripts/env-set.ts`, `print-env.ts`, `discover-access.ts`, `access/check-access.ts`), `.env.template`, and tightened `.gitignore` so `.env.local` remains private (0600) while templates commit cleanly.
- Installed dev dependencies (`ts-node`, `glob`, `pg`) and wired new package scripts (`env:set`, `env:print`, `access:discover`, `access:check`).
- Captured baseline secrets in `.env.local` (`NODE_ENV`, `LIDL_DEVICE_SCALE_FACTOR`); remaining tokens flagged in `docs/ACCESS.md`.
- Scraper context from earlier work persists: Lidl adapter stores flyers/pages with unique constraints; hi-res still requires zoom-trigger to fetch 2400px URLs—the deviceScaleFactor flag alone is insufficient.
- Outstanding untracked files: `scraper/stor​age-state.ts`, `scraper/pnpm-lock.yaml` (left for decision—commit or ignore).

## Access Discovery Sweep
- Run `pnpm env:print`, `pnpm access:discover`, `pnpm access:check` before any request; outputs mask secrets automatically.
- `env:print` checks required vars across `.env.local` (0600), shell env, and CI; use `pnpm env:set KEY VALUE` or pipe via `--from-stdin` to populate.
- `access:discover` scans env files, code, workflows, and common CLI configs to list variable NAMES per service.
- `access:check` performs non-destructive probes (Supabase REST, Postgres DSN, GitHub, Vercel) and validates file-based secrets (Lidl storage state JSON).

## Future Glance
- MVP arc: finish real-data plumbing (Supabase schema + RPC), high-DPI flyer capture, search UI backed by live offers, then notifications/lists.
- Post-MVP: resilience hardening (rate limits, retries, schema migrations), admin dashboard, GDPR export/delete automation, daily digest and optional push.

## Purpose & Objectives
- Spespe finds weekly grocery offers across Italian supermarket chains, ranks by blended price/distance, and keeps GDPR-friendly, magic-link auth.
- Current sprint: finalize data ingestion tooling, ensure env/access handoff, and unblock hi-res flyer parsing + Supabase-backed search UI.

## TL;DR (≤10 lines)
- Env tooling + templates added; `.env.local` locked to 0600 with baseline vars.
- New scripts: `pnpm env:set|print`, `pnpm access:discover|check`.
- Dev deps `ts-node`, `glob`, `pg` installed; package scripts updated.
- Docs generated: `docs/ACCESS.md`, `docs/SYSTEM-PROMPT.md`, this handoff.
- Lidl scraper still needs zoom/sniffer to capture signed 2400px images.
- Supabase schema migrations exist; confirm production sync before next ingestion.
- Outstanding files (`scraper/storage-state.ts`, `scraper/pnpm-lock.yaml`) require keep/ignore decision.
- Next actions: populate missing secrets, run access checks, chase hi-res capture, wire RPC data paths, commit scraper artifacts policy.

## Repo & Stack Snapshot
- `src/app` – Next.js App Router pages (`/`, `/search`, `/dashboard`, auth flows).
- `lib/` – Supabase helpers (`supabase/server.ts`, etc.).
- `sql/` – Ordered migrations (001 catalog through 006 flyer metadata).
- `scraper/` – Node/Playwright crawler (`crawler.ts`, `chains/lidl.ts`, storage state JSON, pnpm workspace).
- Tooling additions: `scripts/` (env/access utilities), `docs/`.
- Stack: Next 16 + React 19 + Tailwind 4, Supabase (Postgres + Auth), Playwright-based crawler.

## Services & Access Overview
- Full matrix in `docs/ACCESS.md` with variable names, storage locations, verification commands, and set instructions.

## Env & Secrets (names only)
- Core: `NEXT_PUBLIC_SUPABASE_URL`, `NEXT_PUBLIC_SUPABASE_ANON_KEY`, `SUPABASE_URL`, `SUPABASE_ANON_KEY`, optional `DATABASE_URL`, `SUPABASE_SERVICE_ROLE`.
- Ops: `GITHUB_TOKEN`, `GITHUB_OWNER`, `GITHUB_REPO`, `VERCEL_TOKEN`, `VERCEL_ORG_ID`, `VERCEL_PROJECT_ID`.
- Scraper: `LIDL_STORAGE_STATE`, `LIDL_DEVICE_SCALE_FACTOR`, `USER_AGENT`.
- Messaging: `BREVO_SMTP_HOST`, `BREVO_SMTP_USER`, `BREVO_SMTP_PASS`.
- Geo: `LOCATIONIQ_API_KEY`, `VAPID_PUBLIC_KEY`, `VAPID_PRIVATE_KEY`.
- Monitoring: `SENTRY_DSN`.
- Set via `pnpm env:set KEY VALUE` (pipe secrets) and verify with `pnpm env:print`.

## Data Model & Constraints
- Tables: `chains`, `stores` (with `geom` geography, GiST index), `flyers`, `offers` (FTS `searchable`), `users`, `user_locations`, `lists`, `list_items`.
- Trigger `offers_search_tsv_trg` ensures FTS; RPC `rpc_search_offers` (002 migration) returns ranked offers with blended score.
- Constraints: unique `(flyer_id, page_no)` on `flyer_pages`, `flyers.publication_id`, default RLS policies allow public reads on catalog tables; user tables enforce `auth.uid()` ownership.
- Pending: confirm PostGIS extension enabled (required for distance queries) and re-run migrations in Supabase prod.

## Lidl Scraper Flow
- `scraper/crawler.ts` orchestrates chain adapters; currently Lidl adapter uses Playwright to open viewer, capture up to 6 pages, store `flyer_runs` + `flyer_pages`.
- Logging keys: `adapter:start`, `adapter:discovered`, `lidl: page image status/ready`, `flyer:error` on Supabase upsert.
- State: hi-res URLs require viewer zoom to load signed 2400px assets (current approach logs when spinner clears but still receives 1200px unless DOM zoom triggered).

## Hi-Res Strategy
- Use Playwright to increase zoom (e.g., `page.evaluate` zoom or emulate Ctrl/Cmd `+`) and monitor network events for `rs:fit:2400` URLs; store both standard and hi-res in Supabase without altering signatures.
- Do not rewrite query params; rely on browser to fetch signed links, then persist original response URLs.
- Consider capturing `requestfinished` events filtered by `/rs:fit:24` to store hi-res metadata.

## Implementation Notes
- Dev dependencies added via `pnpm add -D ts-node glob pg`; `pnpm-lock.yaml` updated accordingly.
- `.gitignore` now white-lists `.env.template`/`.env.example`, ignores `secrets/**`, `*.key`.
- `scripts/env-set.ts` enforces masked output; `.env.local` auto-chmod to 0600.
- Device scale factor for scraper controlled via env and referenced in Playwright launch (line in `scraper/chains/lidl.ts`).
- No changes yet to hi-res logic; flagged as next task.

## Workflows & How To Run
- Frontend dev: `pnpm dev` (Next dev server).
- Scraper: `cd scraper && pnpm install && pnpm start` (uses `tsx crawler.ts`; ensure env set).
- Tooling: run `pnpm env:print`, `pnpm access:discover`, `pnpm access:check`.
- CI: GitHub Actions (scraper hourly) expects service role + storage state; confirm `scraper/pnpm-lock.yaml` policy pre-commit.

## Verification Checklist
- [ ] `pnpm env:print` shows ✅ for required vars (no secrets printed).
- [ ] `pnpm access:check` passes Supabase REST/GitHub/Vercel probes (HTTP 200/2xx).
- [ ] Supabase migrations applied (`select * from fly​er_pages limit 1` returns data).
- [ ] Scraper run logs `lidl: page image ready` and persists `flyer_pages` without conflict errors.
- [ ] Vercel production build (Next 16) succeeds (`pnpm build`).

## Open Tasks
- **P1**: Implement Playwright zoom/network sniffer for hi-res flyer capture (DoD: 2400px URLs stored per page, Supabase upserts succeed).
- **P1**: Wire search UI to `rpc_search_offers`, remove mocks, ensure blended/price/distance toggles propagate mode param.
- **P2**: Finalize secrets population (`docs/ACCESS.md` next-step bullets) and commit stance on `scraper/storage-state.ts` (ignore vs commit).
- **P2**: Add GitHub Action for env checks / scraper run with new tooling (ensures `pnpm access:check` passes in CI).
- **P3**: Document Lidl adapter tests + add integration coverage for duplicate avoidance and hi-res fallback.

## Risks & Ethics
- Scraping flyers may trigger anti-bot; respect robots, throttle, identify contact email.
- Signed hi-res URLs must remain private; avoid sharing or hotlinking.
- Store user geolocation with explicit consent; provide delete/export (GDPR).
- Keep service-role key out of client bundle; restrict to Actions/secrets.

## Appendix
- Sample SQL: `select flyer_id, count(*) from flyer_pages group by 1 order by flyer_id;`
- Env tooling usage: `printf '%s' "$VALUE" | pnpm env:set KEY --from-stdin`; verify with `pnpm env:print`.
- Scraper hi-res recipe: use Playwright `page.mouse.wheel` + keyboard zoom to trigger viewer hi-res fetch, capture via `page.on("requestfinished")`.

## Boot Sequence for New Session
1. `pnpm install` (root) and `cd scraper && pnpm install` if packages changed.
2. `pnpm env:print` → fill missing vars (`pnpm env:set …`).
3. `pnpm access:discover` + `pnpm access:check` for sanity.
4. Validate Supabase connectivity (`pnpm access:check` HTTP codes).
5. Run `pnpm dev` (frontend) and optional `cd scraper && pnpm start -- --max-pages=2` for smoke test.
6. When coding, keep `.env.local` 0600; never commit secrets; run lint/build before PR.
